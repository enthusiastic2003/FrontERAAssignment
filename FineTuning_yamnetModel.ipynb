{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 01:35:10.377289: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-23 01:35:10.385024: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-23 01:35:10.395863: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-23 01:35:10.395884: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 01:35:10.402945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-23 01:35:10.795763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from a csv file\n",
    "\n",
    "csv_file = \"dataClassified/data.csv\"\n",
    "\n",
    "# csv is in format: filename, label\n",
    "\n",
    "prepend_path = \"dataClassified/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 01:35:11.454318: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-23 01:35:11.477506: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load datasets from util.Dataset\n",
    "\n",
    "#dataset_train = util.CustomDataset(csv_file, prepend_path, \"train\")\n",
    "\n",
    "dataset_val = util.CustomDataset(csv_file, prepend_path, \"val\")\n",
    "dataset_test = util.CustomDataset(csv_file, prepend_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = util.CustomDataset(csv_file, prepend_path, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetuning_utils as ftu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 01:38:22.731182: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "embeds, labels = ftu.preprocess_dataset(dataset_train, yamnet_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 01:38:38.652909: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "embeds_val, labels_val = ftu.preprocess_dataset(dataset_val, yamnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 01:38:43.655921: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "embeds_test, labels_test = ftu.preprocess_dataset(dataset_test, yamnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(1024,)),  # YAMNet embeddings are 1024-dimensional\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(util.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Optimizer: Adam with tuned learning rate\n",
    "optimizer = Adam(learning_rate=1e-4 , weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training fold 1/5...\n",
      "Epoch 1/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7116 - loss: 0.6801 - val_accuracy: 0.7651 - val_loss: 0.5569 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7730 - loss: 0.5342 - val_accuracy: 0.7711 - val_loss: 0.5423 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7885 - loss: 0.5007 - val_accuracy: 0.7649 - val_loss: 0.5711 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8088 - loss: 0.4616 - val_accuracy: 0.7606 - val_loss: 0.5796 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8220 - loss: 0.4255 - val_accuracy: 0.7700 - val_loss: 0.5900 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8507 - loss: 0.3773 - val_accuracy: 0.7658 - val_loss: 0.6080 - learning_rate: 5.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8610 - loss: 0.3505 - val_accuracy: 0.7592 - val_loss: 0.6405 - learning_rate: 5.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8734 - loss: 0.3274 - val_accuracy: 0.7584 - val_loss: 0.6556 - learning_rate: 5.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8861 - loss: 0.2933 - val_accuracy: 0.7568 - val_loss: 0.6960 - learning_rate: 2.5000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8953 - loss: 0.2751 - val_accuracy: 0.7529 - val_loss: 0.7281 - learning_rate: 2.5000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9030 - loss: 0.2561 - val_accuracy: 0.7551 - val_loss: 0.7425 - learning_rate: 2.5000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9128 - loss: 0.2388 - val_accuracy: 0.7494 - val_loss: 0.7723 - learning_rate: 1.2500e-05\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step\n",
      "✅ Fold 1 Validation Accuracy: 0.7711\n",
      "\n",
      "🔄 Training fold 2/5...\n",
      "Epoch 1/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7870 - loss: 0.5013 - val_accuracy: 0.7717 - val_loss: 0.5482 - learning_rate: 1.2500e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7981 - loss: 0.4855 - val_accuracy: 0.7711 - val_loss: 0.5505 - learning_rate: 1.2500e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7977 - loss: 0.4845 - val_accuracy: 0.7766 - val_loss: 0.5498 - learning_rate: 1.2500e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 0.4765 - val_accuracy: 0.7715 - val_loss: 0.5513 - learning_rate: 1.2500e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.4702 - val_accuracy: 0.7735 - val_loss: 0.5552 - learning_rate: 6.2500e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8067 - loss: 0.4645 - val_accuracy: 0.7731 - val_loss: 0.5560 - learning_rate: 6.2500e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8080 - loss: 0.4642 - val_accuracy: 0.7700 - val_loss: 0.5602 - learning_rate: 6.2500e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8082 - loss: 0.4631 - val_accuracy: 0.7725 - val_loss: 0.5536 - learning_rate: 3.1250e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8090 - loss: 0.4597 - val_accuracy: 0.7733 - val_loss: 0.5536 - learning_rate: 3.1250e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8118 - loss: 0.4573 - val_accuracy: 0.7725 - val_loss: 0.5565 - learning_rate: 3.1250e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4574 - val_accuracy: 0.7725 - val_loss: 0.5581 - learning_rate: 1.5625e-06\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step\n",
      "✅ Fold 2 Validation Accuracy: 0.7717\n",
      "\n",
      "🔄 Training fold 3/5...\n",
      "Epoch 1/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7934 - loss: 0.4913 - val_accuracy: 0.7717 - val_loss: 0.5472 - learning_rate: 1.5625e-06\n",
      "Epoch 2/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4877 - val_accuracy: 0.7717 - val_loss: 0.5472 - learning_rate: 1.5625e-06\n",
      "Epoch 3/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7932 - loss: 0.4900 - val_accuracy: 0.7705 - val_loss: 0.5499 - learning_rate: 1.5625e-06\n",
      "Epoch 4/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7916 - loss: 0.4919 - val_accuracy: 0.7729 - val_loss: 0.5479 - learning_rate: 1.5625e-06\n",
      "Epoch 5/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7976 - loss: 0.4850 - val_accuracy: 0.7739 - val_loss: 0.5472 - learning_rate: 1.0000e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7953 - loss: 0.4868 - val_accuracy: 0.7729 - val_loss: 0.5485 - learning_rate: 1.0000e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7959 - loss: 0.4863 - val_accuracy: 0.7745 - val_loss: 0.5483 - learning_rate: 1.0000e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4848 - val_accuracy: 0.7743 - val_loss: 0.5490 - learning_rate: 1.0000e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7987 - loss: 0.4797 - val_accuracy: 0.7743 - val_loss: 0.5491 - learning_rate: 1.0000e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8005 - loss: 0.4777 - val_accuracy: 0.7741 - val_loss: 0.5492 - learning_rate: 1.0000e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7944 - loss: 0.4859 - val_accuracy: 0.7748 - val_loss: 0.5487 - learning_rate: 1.0000e-06\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step\n",
      "✅ Fold 3 Validation Accuracy: 0.7717\n",
      "\n",
      "🔄 Training fold 4/5...\n",
      "Epoch 1/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7942 - loss: 0.4892 - val_accuracy: 0.7717 - val_loss: 0.5481 - learning_rate: 1.0000e-06\n",
      "Epoch 2/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7962 - loss: 0.4848 - val_accuracy: 0.7715 - val_loss: 0.5484 - learning_rate: 1.0000e-06\n",
      "Epoch 3/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7950 - loss: 0.4879 - val_accuracy: 0.7719 - val_loss: 0.5490 - learning_rate: 1.0000e-06\n",
      "Epoch 4/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7947 - loss: 0.4874 - val_accuracy: 0.7723 - val_loss: 0.5489 - learning_rate: 1.0000e-06\n",
      "Epoch 5/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7923 - loss: 0.4863 - val_accuracy: 0.7719 - val_loss: 0.5490 - learning_rate: 1.0000e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7948 - loss: 0.4890 - val_accuracy: 0.7719 - val_loss: 0.5490 - learning_rate: 1.0000e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7974 - loss: 0.4867 - val_accuracy: 0.7727 - val_loss: 0.5495 - learning_rate: 1.0000e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7970 - loss: 0.4826 - val_accuracy: 0.7711 - val_loss: 0.5497 - learning_rate: 1.0000e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7938 - loss: 0.4879 - val_accuracy: 0.7713 - val_loss: 0.5501 - learning_rate: 1.0000e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7945 - loss: 0.4854 - val_accuracy: 0.7713 - val_loss: 0.5502 - learning_rate: 1.0000e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7930 - loss: 0.4889 - val_accuracy: 0.7727 - val_loss: 0.5498 - learning_rate: 1.0000e-06\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step\n",
      "✅ Fold 4 Validation Accuracy: 0.7717\n",
      "\n",
      "🔄 Training fold 5/5...\n",
      "Epoch 1/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7912 - loss: 0.4945 - val_accuracy: 0.7711 - val_loss: 0.5476 - learning_rate: 1.0000e-06\n",
      "Epoch 2/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7978 - loss: 0.4864 - val_accuracy: 0.7723 - val_loss: 0.5472 - learning_rate: 1.0000e-06\n",
      "Epoch 3/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7969 - loss: 0.4873 - val_accuracy: 0.7725 - val_loss: 0.5476 - learning_rate: 1.0000e-06\n",
      "Epoch 4/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7954 - loss: 0.4848 - val_accuracy: 0.7731 - val_loss: 0.5474 - learning_rate: 1.0000e-06\n",
      "Epoch 5/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7935 - loss: 0.4914 - val_accuracy: 0.7735 - val_loss: 0.5474 - learning_rate: 1.0000e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7973 - loss: 0.4844 - val_accuracy: 0.7743 - val_loss: 0.5479 - learning_rate: 1.0000e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7954 - loss: 0.4897 - val_accuracy: 0.7735 - val_loss: 0.5484 - learning_rate: 1.0000e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4833 - val_accuracy: 0.7737 - val_loss: 0.5484 - learning_rate: 1.0000e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7966 - loss: 0.4830 - val_accuracy: 0.7731 - val_loss: 0.5486 - learning_rate: 1.0000e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8003 - loss: 0.4802 - val_accuracy: 0.7731 - val_loss: 0.5492 - learning_rate: 1.0000e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7949 - loss: 0.4857 - val_accuracy: 0.7741 - val_loss: 0.5491 - learning_rate: 1.0000e-06\n",
      "Epoch 12/50\n",
      "\u001b[1m1039/1039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7964 - loss: 0.4808 - val_accuracy: 0.7733 - val_loss: 0.5495 - learning_rate: 1.0000e-06\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step\n",
      "✅ Fold 5 Validation Accuracy: 0.7723\n",
      "\n",
      "🎯 Final Validation Accuracy (across folds): 0.7717 ± 0.0004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "\n",
    "# Prepare KFold splitter\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Arrays to store fold results\n",
    "fold_accuracies = []\n",
    "\n",
    "# Convert labels to numpy if not already\n",
    "labels_np = np.array(labels)\n",
    "embeds_np = np.array(embeds)\n",
    "\n",
    "# K-Fold Cross Validation (using separate validation set)\n",
    "for fold, (train_idx, _) in enumerate(kf.split(embeds_np)):\n",
    "    print(f\"\\n🔄 Training fold {fold + 1}/{k_folds}...\")\n",
    "\n",
    "    # Split data for training (validation is separate)\n",
    "    X_train, y_train = embeds_np[train_idx], labels_np[train_idx]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(embeds_val, labels_val),  # Dedicated validation set\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stop, reduce_lr],\n",
    "                        verbose=1,\n",
    "    )\n",
    "    #                    class_weight=class_weights)\n",
    "\n",
    "    # Evaluate on the dedicated validation set\n",
    "    val_preds = model.predict(embeds_val)\n",
    "    val_preds_classes = np.argmax(val_preds, axis=1)\n",
    "    val_accuracy = accuracy_score(labels_val, val_preds_classes)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    print(f\"✅ Fold {fold + 1} Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final Cross-Validation Result\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f\"\\n🎯 Final Validation Accuracy (across folds): {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Check if tensorflow is using GPU\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 - 0s - 862us/step - accuracy: 0.7864 - loss: 0.5391\n",
      "\n",
      "Test accuracy: 0.7863543629646301\n"
     ]
    }
   ],
   "source": [
    "# Check the model performance on the test set\n",
    "\n",
    "test_loss, test_acc = model.evaluate(embeds_test, labels_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the model to a file\n",
    "model.save(\"yamnet_finetuned.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FrontEra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
